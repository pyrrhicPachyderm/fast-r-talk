<<child="theme.rnw">>=
@

%NB: Need to use [fragile] on frames with R chunks, or line breaks in printed code vanish.
%Also on frames with # in \url.

\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{texlogos}

<<benchmark_function, include=F>>=
	benchmark <- function(expr) {
		cat(system.time(expr)[["elapsed"]])
	}
@

\title{Writing Faster R}
\author{
	Christopher Brown
}
\date{}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}
\note[itemize]{
	\item
	There are a lot of us here who have to run code that takes a long time.
	We can save a lot of time by making it faster.
	
	\item
	R isn't naturally a fast language.
	But small changes can make a lot of difference.
	Just last week, I cut a program down from over 60 hours to just 1 hour by rewriting just one arithmetic function in it.
	
	\item
	We'll leap in with an example, to prove a point.
}

\begin{frame}[fragile]{Row Sums: Which is Faster?}
	<<>>=
		row_sums_apply <- function(mat) {
			apply(mat, 1, sum)
		}
	@
	
	<<>>=
		row_sums_loop <- function(mat) {
			result <- rep(0, nrow(mat))
			for(i in 1:ncol(mat)) {
				result <- result + mat[,i]
			}
			return(result)
		}
	@
\end{frame}
\note[itemize]{
	\item
	Explain both pieces of code.
	
	\item
	First uses apply, where 1 means apply over rows.
	
	\item
	Second creates an empty column with \texttt{rep}, then loops over each column and adds it on.
	
	\item
	Ask audience opinion on which will be faster.
	Get a poll.
}

\begin{frame}[fragile]{Row Sums: Benchmarks}
	\begin{uncoverenv}<+->
		<<>>=
			n <- 10000
			random_mat <- matrix(rnorm(n*n), nrow = n, ncol = n)
		@
	\end{uncoverenv}
	
	\begin{uncoverenv}<+->
		<<>>=
			benchmark(row_sums_apply(random_mat))
		@
		
		<<>>=
			benchmark(row_sums_loop(random_mat))
		@
	\end{uncoverenv}
	
	\begin{uncoverenv}<+->
		<<>>=
			benchmark(rowSums(random_mat))
		@
	\end{uncoverenv}
\end{frame}
\note[itemize]{
	\item
	Explain that we've created some dummy data.
	
	\item
	Counter to popular wisdom, the loop is faster.
	Popular wisdom is not wrong; loops are \emph{usually} slower.
	
	\item
	Will explain why this case is different later.
	For now, note that it pays to know what's going on under the hood.
}

\begin{frame}{Compiled or Interpreted}
	\begin{itemize}
		\item<+-> Compiled:
		\begin{itemize}
			\item C, {\cpluspluslogo}, C\#
			\item Rust
			\item Fortran
			\item Go
		\end{itemize}
		\item<.-> Interpreted:
		\begin{itemize}
			\item R
			\item Python
			\item MATLAB
			\item sh, bash, csh, zsh, etc.
		\end{itemize}
		\item<+-> Just-in-time compiled:
		\begin{itemize}
			\item Julia
			\item Java
		\end{itemize}
	\end{itemize}
\end{frame}
\note[itemize]{
	\item
	Explain compiled and interpreted.
	
	\item
	These are just some examples.
	
	\item
	In truth, this is kind of a continuum, and you can sometimes compile interpreted languages or vice versa.
	Just-in-time compiling is mid-way along this continuum, and Java and Julia each do it differently.
}

\begin{frame}[fragile]{What is Vectorising?}
	\begin{itemize}
		\item
		Running loops in C.
		
		\item
		See the source code of \texttt{colSums}:\\
		\url{https://github.com/wch/r-source/blob/b59f3f1a979ae4aeef87263384335b4e59b042b9/src/main/array.c#L1877}
	\end{itemize}
\end{frame}
\note[itemize]{
	\item
	You may well have heard the aphorism to vectorise your code.
	Why does this make things faster?
	
	\item
	It just runs the loops in C instead.
	
	\item
	See the source code for \texttt{colSums}.
	It's complicated, but at its core it's just a loop.
}

\begin{frame}[fragile]{What is \emph{Not} Vectorising?}
	\begin{itemize}
		\item
		\texttt{apply}, \texttt{lapply}, \texttt{sapply}, \texttt{replicate}, etc.
		
		\item
		See the source code of \texttt{lapply}:\\
		\url{https://github.com/wch/r-source/blob/79298c499218846d14500255efd622b5021c10ec/src/main/apply.c#L34}
	\end{itemize}
\end{frame}
\note[itemize]{
	\item
	Have a vectorised \emph{interface}, such as using \texttt{lapply}, is not sufficient to speed up.
	
	\item
	See that the \texttt{lapply} source code just calls an R function again.
}

\begin{frame}[fragile]{Not Vectorising: An Example}
	<<>>=
		loop_apply <- function(x, func) {
			result <- rep(0, length(x))
			for(i in 1:length(x)) {
				result[i] <- func(x[i])
			}
			return(result)
		}
	@
\end{frame}
\note[itemize]{
	\item
	Have a vectorised \emph{interface}, such as using \texttt{lapply}, is not sufficient to speed up.
	
	\item
	Here's \texttt{lapply}, rewritten using an R loop.
}

\begin{frame}[fragile]{Not Vectorising: Benchmarks}
	<<>>=
		n <- 1000000
		random_vec <- rnorm(n)
		dummy_function <- function(x) {x^2 - 1}
	@
	
	<<>>=
		benchmark(loop_apply(random_vec, dummy_function))
	@
	
	<<>>=
		benchmark(lapply(random_vec, dummy_function))
	@
	
	<<>>=
		benchmark(dummy_function(random_vec))
	@
\end{frame}
\note[itemize]{
	\item
	\texttt{loop\_apply} is actually slighty faster than \texttt{lapply}.
	
	\item
	But both are around the same speed, and vastly slower than vectorising properly.
	
	\item
	You can't always vectorise, though.
	You need a loop if results depend on previous results, e.g.\ Markov Chain Monte Carlo.
}

\begin{frame}[fragile]{Profiling: Timing Functions}
	\begin{uncoverenv}<+->
		<<>>=
			system.time(lapply(random_vec, dummy_function))
		@
	\end{uncoverenv}
	
	\begin{uncoverenv}<+->
		<<ref.label='benchmark_function', eval=FALSE>>=
		@
	\end{uncoverenv}
\end{frame}
\note[itemize]{
	\item
	You can spend a long time messing about with your code trying to make it faster.
	And you can make code a lot less readable by trying to make it faster.
	How do we save time and readability?
	
	\item
	Most of your code takes no time at all to run.
	Usually, only one bit is slow.
	Which bit?
	Try profiling.
	
	\item
	You can time a pice of code with \texttt{system.time}.
	User and system time are too complicated to explain; use elapsed.
	The \texttt{benchmark} function I've been using is just pulling the elapsed time from \texttt{system.time}.
	
	\item
	There are several packages with alternatives, e.g.\ \texttt{microbenchmark}.
}

\begin{frame}[fragile]{Profiling: Using a Profiler}
	<<>>=
		outer <- function() {
			profvis::pause(1); inner()
		}
		inner <- function() {
			profvis::pause(2)
		}
		other <- function() {
			profvis::pause(0.5)
		}
	@
	
	<<>>=
		tmp <- tempfile()
		Rprof(tmp)
		result <- outer()
		other_result <- other()
		Rprof()
	@
\end{frame}
\note[itemize]{
	\item
	A profiler will look at your whole code and tell you what's slowest.
	
	\item
	\texttt{Rprof} is included by default; not even a library.
	\texttt{tempfile} just gives a temporary file.
	
	\item
	I'm using \texttt{profvis::pause} because \texttt{Sys.sleep} dosn't use processor time, so doesn't show up to profilers.
	
	\item
	There are other profiling tools, like \texttt{profr}, by Hadley Wickham, and \texttt{profvis}, which gives an interactive visualisation.
	I've never used them, so I can't recommend any one in particular; shop around.
}

\begin{frame}[fragile]{Profiling: Results}
	<<>>=
		summaryRprof(tmp)$by.self
	@
\end{frame}
\note[itemize]{
	\item
	Explain self time vs total time.
	
	\item
	There's output other than \texttt{by.self}, but that's cluttered up with \texttt{knitr} junk, so I can't show that.
	\texttt{by.self} should have all the important stuff.
}

\end{document}
