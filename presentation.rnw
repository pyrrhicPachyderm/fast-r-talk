<<child="theme.rnw">>=
@

%NB: Need to use [fragile] on frames with R chunks, or line breaks in printed code vanish.
%Also on frames with # in \url.

\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{texlogos}

<<benchmark_function, include=F>>=
	benchmark <- function(expr) {
		cat(system.time(expr)[["elapsed"]])
	}
@

\title{Writing Faster R}
\author{
	Christopher Brown
}
\date{}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}
\note[itemize]{
	\item
	There are a lot of us here who have to run code that takes a long time.
	We can save a lot of time by making it faster.
	
	\item
	R isn't naturally a fast language.
	But small changes can make a lot of difference.
	Just last week, I cut a program down from over 60 hours to just 1 hour by rewriting just one arithmetic function in it.
	
	\item
	There'll be lots of code, so I hope you've got your thinking caps on.
	I'll do my best to explain it all, but feel free to stop me at any time.
	
	\item
	We'll leap in with an example, to prove a point.
}

\begin{frame}[fragile]{Row Sums: Which is Faster?}
	<<row_sums_apply>>=
		row_sums_apply <- function(mat) {
			apply(mat, 1, sum)
		}
	@
	
	<<row_sums_loop>>=
		row_sums_loop <- function(mat) {
			result <- rep(0, nrow(mat))
			for(i in 1:ncol(mat)) {
				result <- result + mat[,i]
			}
			return(result)
		}
	@
\end{frame}
\note[itemize]{
	\item
	Explain both pieces of code.
	
	\item
	First uses apply, where 1 means apply over rows.
	
	\item
	Second creates an empty column with \texttt{rep}, then loops over each column and adds it on.
	
	\item
	Ask audience opinion on which will be faster.
	Get a poll.
}

\begin{frame}[fragile]{Row Sums: Benchmarks}
	\begin{uncoverenv}<+->
		<<row_sums_benchmark_setup>>=
			n <- 10000
			random_mat <- matrix(rnorm(n*n), nrow = n, ncol = n)
		@
	\end{uncoverenv}
	
	\begin{uncoverenv}<+->
		<<row_sums_apply_benchmark>>=
			benchmark(row_sums_apply(random_mat))
		@
		
		<<row_sums_loop_benchmark>>=
			benchmark(row_sums_loop(random_mat))
		@
	\end{uncoverenv}
	
	\begin{uncoverenv}<+->
		<<row_sums_builtin_benchmark>>=
			benchmark(rowSums(random_mat))
		@
	\end{uncoverenv}
\end{frame}
\note[itemize]{
	\item
	Explain that we've created some dummy data.
	
	\item
	Counter to popular wisdom, the loop is faster.
	Popular wisdom is not wrong; loops are \emph{usually} slower.
	
	\item
	Will explain why this case is different later.
	For now, note that it pays to know what's going on under the hood.
}

\begin{frame}{Compiled or Interpreted}
	\begin{itemize}
		\item<+-> Compiled:
		\begin{itemize}
			\item C, {\cpluspluslogo}, C\#
			\item Rust
			\item Fortran
			\item Go
		\end{itemize}
		\item<.-> Interpreted:
		\begin{itemize}
			\item R
			\item Python
			\item MATLAB
			\item sh, bash, csh, zsh, etc.
		\end{itemize}
		\item<+-> Just-in-time compiled:
		\begin{itemize}
			\item Julia
			\item Java
		\end{itemize}
	\end{itemize}
\end{frame}
\note[itemize]{
	\item
	Explain compiled and interpreted.
	
	\item
	These are just some examples.
	
	\item
	In truth, this is kind of a continuum, and you can sometimes compile interpreted languages or vice versa.
	Just-in-time compiling is mid-way along this continuum, and Java and Julia each do it differently.
}

\begin{frame}[fragile]{What is Vectorising?}
	\begin{itemize}
		\item
		Running loops in C.
		
		\item
		See the source code of \texttt{colSums}:\\
		\url{https://github.com/wch/r-source/blob/b59f3f1a979ae4aeef87263384335b4e59b042b9/src/main/array.c#L1877}
	\end{itemize}
\end{frame}
\note[itemize]{
	\item
	You may well have heard the aphorism to vectorise your code.
	Why does this make things faster?
	
	\item
	It just runs the loops in C instead.
	
	\item
	See the source code for \texttt{colSums}.
	It's complicated, but at its core it's just a loop.
}

\begin{frame}[fragile]{What is \emph{Not} Vectorising?}
	\begin{itemize}
		\item
		\texttt{apply}, \texttt{lapply}, \texttt{sapply}, \texttt{replicate}, etc.
		
		\item
		See the source code of \texttt{lapply}:\\
		\url{https://github.com/wch/r-source/blob/79298c499218846d14500255efd622b5021c10ec/src/main/apply.c#L34}
	\end{itemize}
\end{frame}
\note[itemize]{
	\item
	Have a vectorised \emph{interface}, such as using \texttt{lapply}, is not sufficient to speed up.
	
	\item
	See that the \texttt{lapply} source code just calls an R function again.
}

\begin{frame}[fragile]{Not Vectorising: An Example}
	<<>>=
		loop_apply <- function(x, func) {
			result <- rep(0, length(x))
			for(i in 1:length(x)) {
				result[i] <- func(x[i])
			}
			return(result)
		}
	@
\end{frame}
\note[itemize]{
	\item
	Have a vectorised \emph{interface}, such as using \texttt{lapply}, is not sufficient to speed up.
	
	\item
	Here's \texttt{lapply}, rewritten using an R loop.
}

\begin{frame}[fragile]{Not Vectorising: Benchmarks}
	<<>>=
		n <- 1000000
		random_vec <- rnorm(n)
		dummy_func <- function(x) {x^2 - 1}
	@
	
	<<>>=
		benchmark(loop_apply(random_vec, dummy_func))
	@
	
	<<>>=
		benchmark(lapply(random_vec, dummy_func))
	@
	
	<<>>=
		benchmark(dummy_func(random_vec))
	@
\end{frame}
\note[itemize]{
	\item
	\texttt{loop\_apply} is actually slighty faster than \texttt{lapply}.
	
	\item
	But both are around the same speed, and vastly slower than vectorising properly.
	
	\item
	You can't always vectorise, though.
	You need a loop if results depend on previous results, e.g.\ Markov Chain Monte Carlo.
}

\begin{frame}[fragile]{Profiling: Timing Functions}
	\begin{uncoverenv}<+->
		<<>>=
			system.time(lapply(random_vec, dummy_func))
		@
	\end{uncoverenv}
	
	\begin{uncoverenv}<+->
		<<ref.label='benchmark_function', eval=FALSE>>=
		@
	\end{uncoverenv}
\end{frame}
\note[itemize]{
	\item
	You can spend a long time messing about with your code trying to make it faster.
	And you can make code a lot less readable by trying to make it faster.
	How do we save time and readability?
	
	\item
	Most of your code takes no time at all to run.
	Usually, only one bit is slow.
	Which bit?
	Try profiling.
	
	\item
	You can time a pice of code with \texttt{system.time}.
	User and system time are too complicated to explain; use elapsed.
	The \texttt{benchmark} function I've been using is just pulling the elapsed time from \texttt{system.time}.
	
	\item
	There are several packages with alternatives, e.g.\ \texttt{microbenchmark}.
}

\begin{frame}[fragile]{Profiling: Using a Profiler}
	<<>>=
		outer <- function() {
			profvis::pause(1); inner()
		}
		inner <- function() {
			profvis::pause(2)
		}
		other <- function() {
			profvis::pause(0.5)
		}
	@
	
	<<>>=
		tmp <- tempfile()
		Rprof(tmp)
		result <- outer()
		other_result <- other()
		Rprof()
	@
\end{frame}
\note[itemize]{
	\item
	A profiler will look at your whole code and tell you what's slowest.
	
	\item
	\texttt{Rprof} is included by default; not even a library.
	\texttt{tempfile} just gives a temporary file.
	
	\item
	I'm using \texttt{profvis::pause} because \texttt{Sys.sleep} dosn't use processor time, so doesn't show up to profilers.
	
	\item
	There are other profiling tools, like \texttt{profr}, by Hadley Wickham, and \texttt{profvis}, which gives an interactive visualisation.
	I've never used them, so I can't recommend any one in particular; shop around.
}

\begin{frame}[fragile]{Profiling: Results}
	<<>>=
		summaryRprof(tmp)$by.self
	@
\end{frame}
\note[itemize]{
	\item
	Explain self time vs total time.
	
	\item
	There's output other than \texttt{by.self}, but that's cluttered up with \texttt{knitr} junk, so I can't show that.
	\texttt{by.self} should have all the important stuff.
}

\begin{frame}[fragile]{Compiling}
	<<fib>>=
		fib <- function(n) {
			result <- rep(0, n)
			result[1] <- 1
			result[2] <- 1
			for(i in 3:n) {
				result[i] <- result[i-1] + result[i-2]
			}
			return(result)
		}
	@
	
	<<>>=
		fib_byte <- compiler::cmpfun(fib)
	@
\end{frame}
\note[itemize]{
	\item
	As I mentioned earlier, intepreted languages can sometimes be compiled.
	R comes with a function for that.
	
	\item
	So, once you've identified a bottleneck, the fastest thing to do is just to compile it.
	It doesn't often help much, but it's very little effort to try.
	
	\item
	This just computes the Fibonacci sequence up to $n$.
	I'm using the Fibonacci sequence as an example because it depends on previous results, so it can't be vectorised.
}

\begin{frame}[fragile]{Compiling: Benchmarks}
	\begin{uncoverenv}<+->
		<<>>=
			benchmark(fib(1e5))
		@
		
		<<>>=
			benchmark(fib_byte(1e5))
		@
	\end{uncoverenv}
	\begin{uncoverenv}<+->
		<<>>=
			benchmark(fib(1e7))
		@
		
		<<>>=
			benchmark(fib_byte(1e7))
		@
	\end{uncoverenv}
\end{frame}
\note[itemize]{
	\item
	There's a time saving.
	
	\item
	But it doesn't scale very well.
}

\begin{frame}[fragile]{Rcpp}
	\begin{onlyenv}<+>
		<<ref.label='fib'>>=
		@
	\end{onlyenv}
	
	\begin{onlyenv}<+>
		<<>>=
			Rcpp::cppFunction('IntegerVector fib_cpp(int n) {
				IntegerVector result(n);
				result[0] = 1;
				result[1] = 1;
				for(int i = 2; i < n; i++) {
					result[i] = result[i-1] + result[i-2];
				}
				return result;
			}')
		@
	\end{onlyenv}
\end{frame}
\note[itemize]{
	\item
	Rcpp makes it really easy to write {\cpluspluslogo} straight into R.
	
	\item
	Learning a new language may seem scary, but R and {\cpluspluslogo} are actually far more similar than you might think.
	Compare these two.
	
	\item
	Things to note:
	\begin{itemize}
		\item We have to explicitly declare types.
		This is great; it catches errors and provides built-in documentation.
		
		\item Arrays start at 0, not 1.
		
		\item Semicolons to end lines.
		
		\item For loops work slightly differently.
		You can do R style ones as well, though.
		
		\item No brackets around return statement.
	\end{itemize}
}

\begin{frame}[fragile]{Rcpp: Benchmarks}
	<<>>=
		benchmark(fib(1e7))
	@
	
	<<>>=
		benchmark(fib_byte(1e7))
	@
	
	<<>>=
		benchmark(fib_cpp(1e7))
	@
\end{frame}
\note[itemize]{
	\item
	Now blazing fast.
}

\begin{frame}[fragile]{Memory Reallocation}
	<<>>=
		fib2 <- function(n) {
			result <- c(1, 1)
			for(i in 3:n) {
				result <- c(result, result[i-1] + result[i-2])
			}
			return(result)
		}
	@
\end{frame}
\note[itemize]{
	\item
	Those were some things to try.
	Now, some things to be careful of.
	
	\item
	Can anyone guess how the speed of this code will compare to our earlier Fibonacci code?
}

\begin{frame}[fragile]{Memory Reallocation: Benchmarks}
	<<>>=
		benchmark(fib(1e4))
	@
	
	<<>>=
		benchmark(fib2(1e4))
	@
\end{frame}
\note[itemize]{
	\item
	So, what went wrong?
	
	\item
	Explain memory reallocation.
	
	\item
	Always preallocate your vectors.
	
	\item
	This is one advantage of \texttt{apply} and its cousins; they preallocate for you.
}

\begin{frame}[fragile]{Memory Structure}
	<<ref.label='row_sums_apply'>>=
	@
	
	<<ref.label='row_sums_loop'>>=
	@
\end{frame}
\note[itemize]{
	\item
	Remember these?
}

\begin{frame}[fragile]{Memory Structure: Benchmarks}
	<<ref.label='row_sums_benchmark_setup'>>=
	@
	
	<<ref.label='row_sums_apply_benchmark'>>=
	@
	
	<<ref.label='row_sums_loop_benchmark'>>=
	@
	
	<<ref.label='row_sums_builtin_benchmark'>>=
	@
\end{frame}
\note[itemize]{
	\item
	Remember how the loop I wrote was actually faster than using apply?
	
	\item
	Explain column-major order and cache misses.
	Use a whiteboard.
	
	\item
	In general, try to access things in column order.
	It makes a bigger difference than you think it should.
}

\begin{frame}{The Process}
	\begin{itemize}
		\item<+-> Where is it slow?
		\begin{itemize}
			\item Profile
			\item Is it worth my time?
		\end{itemize}
		\item<+-> Is it doing unnecessary work?
		\begin{itemize}
			\item Pre-allocate
			\item Cache thrashing
			\item Algorithmic efficiency
		\end{itemize}
		\item<+-> Vectorise it
		\begin{itemize}
			\item Is there are a base function?
			\item Is there a package?
		\end{itemize}
		\item<+-> Compile it
		\begin{itemize}
			\item \texttt{compiler::cmpfun}
			\item Rcpp
		\end{itemize}
	\end{itemize}
\end{frame}
\note[itemize]{
	\item
	I've not gone over algorithmic efficiency here.
	In truth, it's the most important thing of all.
	But it's hard to teach without a full-blown course.
	Any algorithmic optimisations are made case-by-case; general rules are hard.
	Explain big O notation if there's time.
}

\end{document}
